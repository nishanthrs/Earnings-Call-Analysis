{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import spacy\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "# TODO: Continue to remove more stopwords (i.e. company name, 'quarter', 'year', '%', etc.)\n",
    "new_stopwords = ['thank', 'thanks', 'think', 'quarter', 'year', 'â€™s',\n",
    "                 '%']  # People say 'thank you' way too much in earnings calls!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_stopwords(stopwords, new_stopwords):\n",
    "    \"\"\"\n",
    "    Method that adds new stopwords to list of spacy default stopwords\n",
    "\n",
    "    Args:\n",
    "    stopwords - spacy default stopwords\n",
    "    new_stopwords - new stopwords\n",
    "\n",
    "    Returns: new list of stopwords\n",
    "    \"\"\"\n",
    "\n",
    "    for new_stopword in new_stopwords:\n",
    "        stopwords.add(new_stopword)\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_clean_transcript(local_file, new_stopwords):\n",
    "    \"\"\"\n",
    "    Method that implements part of NLP pipeline of cleaning text:\n",
    "    1. Tokenization\n",
    "    2. Removing stopwords (commonly known stopwords, pronouns, keywords with little info, words less than 2 chars)\n",
    "    3. Lemmatization\n",
    "\n",
    "    Args: \n",
    "    local_file - name of earnings call transcript file\n",
    "    new_stopwords - new stop words to filter doc with\n",
    "\n",
    "    Returns: list of tokens (spacy doc) and list of lemmatized words (list of strings)\n",
    "    \"\"\"\n",
    "\n",
    "    stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    stopwords = add_stopwords(stopwords, new_stopwords)\n",
    "\n",
    "    transcript_read = open(local_file, 'r')\n",
    "    transcript_doc = nlp(transcript_read.read())\n",
    "    \n",
    "    # Clean spacy doc and structure as 2D numpy array of spacy tokens\n",
    "    cleaned_transcript_doc = []\n",
    "    for sent in transcript_doc.sents:\n",
    "        cleaned_transcript_sent = []\n",
    "        for token in sent:\n",
    "            token_text = token.lemma_.lower().strip()\n",
    "            if token_text not in stopwords and token_text not in punctuation and len(token_text) > 2 and token.lemma_ != '-PRON-':\n",
    "                cleaned_transcript_sent.append(token)\n",
    "        \n",
    "        if len(cleaned_transcript_sent) > 0:\n",
    "            cleaned_transcript_doc.append(cleaned_transcript_sent)\n",
    "    \n",
    "    return cleaned_transcript_doc\n",
    "\n",
    "def flatten_doc(transcript_doc):\n",
    "    return [token for transcript_sent in transcript_doc for token in transcript_sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_freq_terms(transcript_doc, k):\n",
    "    \"\"\"\n",
    "    Method that get k most frequent words of earnings call transcript\n",
    "\n",
    "    Args: \n",
    "    transcript_doc - spacy doc of transcript\n",
    "    transcript_text - text of transcript\n",
    "    k - k most common frequent terms\n",
    "\n",
    "    Returns: list of k most frequent words\n",
    "    \"\"\"\n",
    "\n",
    "    # print('Unflattened transcript doc: ', transcript_doc)\n",
    "    # transcript_doc = transcript_doc.flatten()\n",
    "    transcript_doc = flatten_doc(transcript_doc)\n",
    "    \n",
    "    # five most common tokens\n",
    "    transcript_text = [token.lemma_.lower().strip() for token in transcript_doc] # Make sure to lemmatize and normalize\n",
    "    word_freq = Counter(transcript_text)\n",
    "    common_words = word_freq.most_common(k)\n",
    "\n",
    "    # five most common noun tokens\n",
    "    nouns = [token.lemma_.lower().strip() for token in transcript_doc if token.pos_ == 'NOUN'] # Make sure to lemmatize and normalize\n",
    "    noun_freq = Counter(nouns)\n",
    "    common_nouns = noun_freq.most_common(k)\n",
    "\n",
    "    # five most common verb tokens\n",
    "    verbs = [token.lemma_.lower().strip() for token in transcript_doc if token.pos_ == 'VERB'] # Make sure to lemmatize and normalize\n",
    "    verb_freq = Counter(verbs)\n",
    "    common_verbs = verb_freq.most_common(k)\n",
    "\n",
    "    return (common_words, common_nouns, common_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_freq_words_to_file(analysis_dir, local_file, common_terms):\n",
    "    \"\"\"\n",
    "    Method that writes the most frequent terms from the document to a file\n",
    "\n",
    "    Args:\n",
    "    analysis_dir - directory of frequent terms of earnings transcripts\n",
    "    local_file - name of file to write frequent terms to\n",
    "    common_terms - list of frequent terms\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(analysis_dir):\n",
    "        os.makedirs(analysis_dir)\n",
    "    freq_words_file_path = analysis_dir + '/' + local_file\n",
    "    freq_words_file = open(freq_words_file_path, 'w+')\n",
    "\n",
    "    for terms in common_terms:\n",
    "        for term in terms:\n",
    "            freq_words_file.write(str(term) + ' ')\n",
    "        freq_words_file.write('\\n')\n",
    "\n",
    "    freq_words_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exec_transcript_analysis(transcript_dir, analysis_dir, transcript_filename):\n",
    "    \"\"\"\n",
    "    Method that executes the entire transcript analysis to get most frequent words\n",
    "\n",
    "    Args: \n",
    "    transcript_dir - directory of transcripts\n",
    "    analysis_dir - directory of transcript analysis files\n",
    "    transcript_filename - transcript filename\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "\n",
    "    transcript_path = transcript_dir + transcript_filename\n",
    "    transcript_doc = tokenize_and_clean_transcript(\n",
    "        transcript_path, new_stopwords)\n",
    "    common_terms = get_freq_terms(transcript_doc, 10)\n",
    "    write_freq_words_to_file(\n",
    "        analysis_dir,\n",
    "        transcript_filename.split('.')[0] + '_freq_words.txt', common_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_dir = '../earnings_call_transcripts/tndm/'\n",
    "analysis_dir = '../nlp_analyis'\n",
    "transcript_filename = 'tndm_earnings_transcript_q3_2018.txt'\n",
    "exec_transcript_analysis(transcript_dir, analysis_dir, transcript_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings:\n",
    "# 1. Elmo (tensorflow_hub doesn't work on this MacOS, need to find another way to load these embeddings)\n",
    "# Generated via bidirectional RNNs by feeding word into this NN\n",
    "'''\n",
    "elmo_embeddings_url = \"https://tfhub.dev/google/elmo/2\"\n",
    "embed = hub.Module(elmo_embeddings_url, trainable=True)\n",
    "'''\n",
    "# 2. Word2Vec\n",
    "# Generated via ...\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_deep_word_embeddings(transcript_doc, embedding_type):\n",
    "    \"\"\"\n",
    "    Method that generates deep word embeddings of tokens in transcript\n",
    "    These embeddings will be used in various NLP tasks:\n",
    "    1. Sentiment analysis\n",
    "    2. Named entity extraction\n",
    "    3. Summarization\n",
    "    4. Question answering\n",
    "    5. Document classification\n",
    "    \n",
    "    Args: \n",
    "    transcript_doc - spacy doc of transcript\n",
    "    transcript_text - text of transcript\n",
    "    embedding_type - embedding framework used (e.g. word2vec, Elmo)\n",
    "\n",
    "    Returns: list of word embeddings\n",
    "    \"\"\"\n",
    "    \n",
    "    if embedding_type == 'elmo':\n",
    "        pass\n",
    "    elif embedding_type == 'word2vec':\n",
    "        transcript_text = [[token.lemma_.lower().strip() for token in sent] for sent in transcript_doc]\n",
    "        word2vec = Word2Vec(transcript_text, min_count=2) # Generates list of numpy vectors\n",
    "        return word2vec\n",
    "\n",
    "transcript_path = '../earnings_call_transcripts/tndm/tndm_earnings_transcript_q3_2018.txt'\n",
    "transcript_doc = tokenize_and_clean_transcript(transcript_path, new_stopwords)\n",
    "word2vec_embeddings = generate_deep_word_embeddings(transcript_doc, 'word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embeddings_vocab = word2vec_embeddings.wv.vocab\n",
    "transcript_word2vec_embeddings = {}\n",
    "for word in embeddings_vocab:\n",
    "    transcript_word2vec_embeddings[word] = word2vec_embeddings.wv[word]\n",
    "print(transcript_word2vec_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize these embeddings on 2D plane using PCA and tSNE\n",
    "sorted_words = sorted(transcript_word2vec_embeddings.keys())\n",
    "sorted_embeddings = []\n",
    "for word in sorted_words:\n",
    "    sorted_embeddings.append(transcript_word2vec_embeddings[word])\n",
    "sorted_embeddings = np.array(sorted_embeddings)\n",
    "\n",
    "# Reduce using PCA first b/c tSNE is quadratic runtime and space (inefficient for datasets with many features)\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=50) # Reduce down to 50 dim\n",
    "compressed_embeddings = pca.fit_transform(sorted_embeddings)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "compressed_embeddings = TSNE(n_components=2).fit_transform(compressed_embeddings) # Further reduce to 2 dim using t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nimport matplotlib.pyplot as plt\\n\\nprint(y)\\nplt.figure(figsize=(50,50))\\nplt.scatter(x=y[:,0], y=y[:,1], label=sorted_words)\\nplt.show()\\n'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use Plotly to generate interactive chart that plots all (dimension-reduced) words in 2D vector space\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "data = [\n",
    "    go.Scatter(\n",
    "        x=[i[0] for i in compressed_embeddings],\n",
    "        y=[i[1] for i in compressed_embeddings],\n",
    "        mode='markers',\n",
    "        text=[i for i in sorted_words],\n",
    "    marker=dict(\n",
    "        size=16,\n",
    "        color = [len(i) for i in sorted_words], # Colored by word length\n",
    "        opacity= 0.8,\n",
    "        colorscale='Viridis',\n",
    "        showscale=False\n",
    "    )\n",
    "    )\n",
    "]\n",
    "layout = go.Layout()\n",
    "layout = dict(\n",
    "              yaxis = dict(zeroline = False),\n",
    "              xaxis = dict(zeroline = False)\n",
    "             )\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "file = plot(fig, filename='TNDM_Q3_2018_Transcript_Word_Encodings.html')\n",
    "\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(y)\n",
    "plt.figure(figsize=(50,50))\n",
    "plt.scatter(x=y[:,0], y=y[:,1], label=sorted_words)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 1. Find out how word2vec embeddings are generated (via context or definition or sentence placement or ...?)\n",
    "# 2. Generate document embeddings next"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
